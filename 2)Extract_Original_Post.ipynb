{"cells":[{"cell_type":"markdown","source":["# Scarica tweets partendo dai 'tweet_id' contenuti negli articoli di google news. Per ogni tweet estrai nuovi 'tweet_id' dalla sezione 'urls'."],"metadata":{"id":"qY5lChRMZ47l"}},{"cell_type":"markdown","source":["##**Scarica le librerie necessarie**"],"metadata":{"id":"5Y-C1rtSacd8"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5216,"status":"ok","timestamp":1655290616641,"user":{"displayName":"Tommaso Strada","userId":"07272469469703036442"},"user_tz":-120},"id":"E9ZDNLejq9AB","outputId":"0583e39d-5aa4-4508-a4ff-f8900e3c4fdf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tweepy in /usr/local/lib/python3.7/dist-packages (3.10.0)\n","Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy) (2.23.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.3.1)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.15.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy) (3.2.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2022.5.18.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (3.0.4)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n"]}],"source":["pip install tweepy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":10409,"status":"ok","timestamp":1655290627040,"user":{"displayName":"Tommaso Strada","userId":"07272469469703036442"},"user_tz":-120},"id":"OI7-8JJeouP5","outputId":"e4e09aca-f8fd-44a0-ef63-d54ab01b337d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting requests_html\n","  Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n","Collecting parse\n","  Downloading parse-1.19.0.tar.gz (30 kB)\n","Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (from requests_html) (0.0.1)\n","Collecting pyquery\n","  Downloading pyquery-1.4.3-py3-none-any.whl (22 kB)\n","Collecting w3lib\n","  Downloading w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\n","Collecting pyppeteer>=0.0.14\n","  Downloading pyppeteer-1.0.2-py3-none-any.whl (83 kB)\n","\u001b[K     |████████████████████████████████| 83 kB 2.2 MB/s \n","\u001b[?25hCollecting fake-useragent\n","  Downloading fake-useragent-0.1.11.tar.gz (13 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from requests_html) (2.23.0)\n","Collecting urllib3<2.0.0,>=1.25.8\n","  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n","\u001b[K     |████████████████████████████████| 138 kB 64.9 MB/s \n","\u001b[?25hCollecting pyee<9.0.0,>=8.1.0\n","  Downloading pyee-8.2.2-py2.py3-none-any.whl (12 kB)\n","Requirement already satisfied: certifi>=2021 in /usr/local/lib/python3.7/dist-packages (from pyppeteer>=0.0.14->requests_html) (2022.5.18.1)\n","Collecting websockets<11.0,>=10.0\n","  Downloading websockets-10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (112 kB)\n","\u001b[K     |████████████████████████████████| 112 kB 86.5 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.7/dist-packages (from pyppeteer>=0.0.14->requests_html) (4.11.4)\n","Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.7/dist-packages (from pyppeteer>=0.0.14->requests_html) (4.64.0)\n","Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.7/dist-packages (from pyppeteer>=0.0.14->requests_html) (1.4.4)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests_html) (4.2.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests_html) (3.8.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4->requests_html) (4.6.3)\n","Collecting cssselect>0.7.9\n","  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.7/dist-packages (from pyquery->requests_html) (4.2.6)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->requests_html) (3.0.4)\n","Collecting urllib3<2.0.0,>=1.25.8\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 55.5 MB/s \n","\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->requests_html) (2.10)\n","Requirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from w3lib->requests_html) (1.15.0)\n","Building wheels for collected packages: fake-useragent, parse\n","  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-py3-none-any.whl size=13502 sha256=2bda03898a6bfad2ece6ac796ba5ddd6bd0c2f7fbad41cc25831bbd48bf01871\n","  Stored in directory: /root/.cache/pip/wheels/ed/f7/62/50ab6c9a0b5567267ab76a9daa9d06315704209b2c5d032031\n","  Building wheel for parse (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for parse: filename=parse-1.19.0-py3-none-any.whl size=24591 sha256=b650561aa56bcac3ec65220cea424651ddfd7c54fe4fab8beee9fa5fa954a7f7\n","  Stored in directory: /root/.cache/pip/wheels/9c/aa/cc/f2228050ccb40f22144b073f15a2c84f11204f29fc0dce028e\n","Successfully built fake-useragent parse\n","Installing collected packages: websockets, urllib3, pyee, cssselect, w3lib, pyquery, pyppeteer, parse, fake-useragent, requests-html\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed cssselect-1.1.0 fake-useragent-0.1.11 parse-1.19.0 pyee-8.2.2 pyppeteer-1.0.2 pyquery-1.4.3 requests-html-0.10.0 urllib3-1.25.11 w3lib-1.22.0 websockets-10.3\n"]}],"source":["pip install requests_html"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","from google.colab import files\n","from google.colab import drive\n","import os\n","import glob\n","\n","# Colleghiamoci a Mydrive\n","drive.mount('/content/drive')\n","\n","# Cartella da cui importare file = '/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/JSON SCRAPING CLEAN TOM'   \n","# Cartella in cui esportare file post twitter = '/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/JSON TWITTER TOM'  \n","\n","# Spostati nella cartella contenente tutti i dataset di scraping\n","path = '/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/JSON SCRAPING CLEAN'\n","os.chdir(path)\n","\n","# Importa tutti i file della cartella\n","# scrape_list = [i for i in glob.glob('*.json')]\n","scrape_list = [i for i in glob.glob('*.json')]\n","\n","# Inseriscili in un dizionario\n","diz = {'{}'.format((m).split(' scraping')[0]): pd.read_json(m, orient='records') for m in scrape_list}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gq3FMVM2objm","executionInfo":{"status":"ok","timestamp":1655290824152,"user_tz":-120,"elapsed":6166,"user":{"displayName":"Tommaso Strada","userId":"07272469469703036442"}},"outputId":"d4113398-ce2c-4203-b571-5a0d0e9bf5cd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aBWoxKOpuvaI"},"outputs":[],"source":["# Per ogni film costruisci una lista contenente tutti i 'tweet_id'\n","dictio_list = {}\n","for n in diz:\n","  dictio_list[n] = []\n","  for row in diz[n]['twitter_id'].values:\n","    f = re.compile('(\\d+)') #\\d serve a prendere i numeri mentre il + le espressioni, metodo per compilare pattern di un espressione \n","    for el in row:\n","        m = f.search(el)\n","        if m:\n","          k = m.group(1)\n","          if k not in dictio_list[n]: \n","            dictio_list[n].append(int(k))\n","          else:\n","              dictio_list[n]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bT9_LtbNQTwZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655290931938,"user_tz":-120,"elapsed":300,"user":{"displayName":"Tommaso Strada","userId":"07272469469703036442"}},"outputId":"6ac22959-12db-46ab-b717-27e2d383a3e2"},"outputs":[{"output_type":"stream","name":"stdout","text":["862 3099 Doctor Strange in the Multiverse of Madness Twitter\n","684 2873 Spider-Man: No Way Home Twitter\n","494 2225 Eternals Twitter\n","122 554 Shang-Chi and the Legend of the Ten Rings Twitter\n","97 607 Black Widow Twitter\n","37 235 Spider-Man: Far From Home Twitter\n","65 435 Avengers: Endgame Twitter\n","57 398 Captain Marvel Twitter\n","7 101 Ant-Man and the Wasp Twitter\n","57 478 Avengers: Infinity War Twitter\n","88 722 Black Panther Twitter\n","14 74 Thor: Ragnarok Twitter\n","17 94 Spider-Man: Homecoming Twitter\n","1 3 Iron Man 3 Twitter\n","3 18 Avengers: Age of Ultron Twitter\n","1 10 Ant-Man Twitter\n","5 44 Captain America: Civil War Twitter\n","1 1 Doctor Strange Twitter\n","1 12 Guardians of Galaxy Vol.2 Twitter\n"]}],"source":["# Controlla quanti tweet_id sono presenti in ogni dataset\n","for n in diz:\n","  print(len(diz[n]), len(dictio_list[n]), n)"]},{"cell_type":"markdown","metadata":{"id":"YMv_A7pT2k9n"},"source":["##**Funzione API Twitter**"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":141156,"status":"ok","timestamp":1655291464619,"user":{"displayName":"Tommaso Strada","userId":"07272469469703036442"},"user_tz":-120},"id":"mBDW0TRKDphT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6662bbee-1b67-4784-95aa-2673f4adaa51"},"outputs":[{"output_type":"stream","name":"stdout","text":["Doctor Strange in the Multiverse of Madness Twitter\n","2189\n","149\n","8\n","Spider-Man: No Way Home Twitter\n","2080\n","150\n","9\n","1\n","1\n","Eternals Twitter\n","1604\n","96\n","6\n","Shang-Chi and the Legend of the Ten Rings Twitter\n","373\n","33\n","1\n","Black Widow Twitter\n","377\n","31\n","6\n","5\n","4\n","3\n","2\n","Spider-Man: Far From Home Twitter\n","144\n","6\n","2\n","Avengers: Endgame Twitter\n","271\n","13\n","1\n","Captain Marvel Twitter\n","256\n","8\n","1\n","Ant-Man and the Wasp Twitter\n","71\n","2\n","Avengers: Infinity War Twitter\n","295\n","3\n","Black Panther Twitter\n","266\n","10\n","Thor: Ragnarok Twitter\n","53\n","4\n","Spider-Man: Homecoming Twitter\n","55\n","5\n","2\n","0\n","Iron Man 3 Twitter\n","3\n","Avengers: Age of Ultron Twitter\n","17\n","Ant-Man Twitter\n","7\n","Captain America: Civil War Twitter\n","27\n","Doctor Strange Twitter\n","1\n","Guardians of Galaxy Vol.2 Twitter\n","11\n"]}],"source":["import pandas as pd\n","import tweepy\n","import datetime\n","import re\n","def Prime(lista):\n","  # Importa le credenziali fornite da Twitter per utilizzare le API\n","  consumer_key = \"uBJISqAHpRkCm79dKDwsBkHL1\"\n","  consumer_secret = \"NHvp1zx4Sk5n15iV4A1v2wK6Ygo7BAgheFOgEyRPmNcBkHthNq\"\n","  access_key = \"1460586759918600195-SR05QtXUgVfk7WjAJFe8RcG3a22XG0\"\n","  access_secret = \"MTwCZtmuHmvJqfxM943VqWq02GZvyeCj0Oflbit1w1y7n\"\n","  auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n","  auth.set_access_token(access_key, access_secret)  \n","  api = tweepy.API(auth, wait_on_rate_limit=True)\n","\n","  def Post(lista):\n","      # Per superare il limite di ricerca di Twitter (pari a 100 tweets), prendi gruppi da 100 elementi alla volta dalla lista dei tweet_id.\n","      dictio = {}     \n","      # Per coprire tutti gli elementi nella lista imposta l+1 gruppi da 100 (es: 2001 righe, round(2001/100)=round(20,01)=20 -> perdi una riga)\n","      for l in range(0,(round((len(lista)/100)+0.5))):   \n","          id_ = lista[100*(l):100*(l+1)]\n","          tweet_obj = {} \n","          # Metodo di ricerca tweets\n","          tweets = api.statuses_lookup(id_, tweet_mode = 'extended', lang = 'en')  \n","          for n in range(11):\n","            tweet_obj[n] = []\n","          for i in range(len(tweets)):\n","            tweet_obj[0].append(tweets[i].user.screen_name)\n","            tweet_obj[1].append(tweets[i].user.id)\n","            tweet_obj[2].append(tweets[i].user.verified)\n","            tweet_obj[3].append(tweets[i].user.created_at)\n","            tweet_obj[4].append(str(tweets[i].id))\n","            tweet_obj[5].append(pd.to_datetime(tweets[i].created_at))\n","            tweet_obj[6].append(tweets[i].user.description)\n","            tweet_obj[7].append(tweets[i].full_text)\n","            tweet_obj[8].append(re.findall(r'#(\\w+)', tweets[i].full_text))   # estrai gli hashtags dal testo\n","            tweet_obj[9].append(tweets[i].source)\n","            tweet_obj[10].append(tweets[i].entities['urls'])\n","\n","# Costruisci il dataframe con i tweets della lista di partenza\n","          f = pd.DataFrame(tweet_obj).rename(columns={0:'Username', 1:'User_id', 2:'User_verified', 3:'User_creation', 4:'Tweet_id', 5:'Date', 6:'User_Description', 7:'Text', 8:'Hashtags', 9:'Source', 10:'Urls'})    \n","          f['User_creation'] = pd.to_datetime(f['User_creation']) \n","          f['User_creation'] = f['User_creation'].dt.strftime('%d/%m/%Y')  # formatta attributo 'User_creation'\n","          f['Date'] = pd.to_datetime(f['Date'])    # formatta attributo 'Date'\n","          f['Date'] = f['Date'].dt.strftime('%d/%m/%Y')\n","          dictio[l] = f\n","          \n","# Concatena tutti i 'l' dataframe         \n","      Original_post = dictio[0]\n","      for i in range(1,(round((len(lista)/100)+0.5))):\n","        Original_post = pd.concat([Original_post, dictio[i]]).sort_values(by = 'Date')\n","\n","# Costruisci un nuovo indice          \n","      z = []\n","      for el in range(1,(len(Original_post)+1)):\n","          z.append(el)\n","\n","      Original_post[\"Id\"]=z\n","      Original_post.reset_index(inplace = True)\n","      del[Original_post['index']]\n","      # Elimina duplicati\n","      Original_post = Original_post.drop_duplicates(subset =\"Tweet_id\", keep = 'first')\n","      Original_post = Original_post.set_index('Id')\n","      print(len(Original_post))\n","      return(Original_post)   \n","\n","  def join_table_function(Original_post):   # costruisci una lista con tutti i tweet_id ottenuti dai link del dataframe precedente\n","    a = Original_post.reset_index()    \n","    b = a['Urls'].to_list()\n","    t = a['Id'].to_list()\n","\n","    h = re.compile('(https://twitter.com)/(\\w+)/(status)/(\\d+)')\n","    tweet_id = []\n","    for row in range(len(a)):\n","      c = str(b[row]).split(', ')   # dividi i vari gruppi di link\n","      for elem in range(len(c)):\n","        d = c[elem].split(': ')     # dividi i vari link\n","        for x in range(len(d)):\n","          m = h.search(d[x])      \n","          if m:\n","            tweet_id.append(m.group(4))\n","\n","      \n","    return(tweet_id)\n","    \n","  post = Post(lista)     # la variabile post è il dataframe ottenuto dall'estrazione tweets (Post) dalla lista di tweet_id (lista)\n","  jt = join_table_function(post)   # la variabile jt è il dataframe ottenuto dall'estrazione tweets_id (join_table_function) dal dataframe tweets (Post(lista))\n","\n","  while len(jt) != 0:\n","    post1 = Post(jt)\n","    jt = join_table_function(post1)\n","    if len(post1) != 0:     # evita che la mancata presenza di tweet blocchi la funzione\n","      if post.iloc[-1]['Tweet_id'] != post1.iloc[0]['Tweet_id']:    # se un tweet rimanda a se stesso, interrompi il ciclo for\n","        post = pd.concat([post, post1], ignore_index = True)\n","      else:\n","        break\n","    else:\n","      break\n","\n","\n","  post.drop_duplicates(subset = 'Tweet_id', inplace = True, keep = 'first')\n","  post.to_json('/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/JSON TWITTER/Final_post_{}.json'.format(n), orient='records')\n","  return(post, jt)\n","\n","\n","for n in diz: \n","  print(n)\n","  if len(diz[n]) != 0:   # per ogni film\n","    lista = dictio_list[n]      # La lista iniziale contiene gli id estratti dagli articoli\n","    Prime(lista)\n","  "]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Extract_Original_Post.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}