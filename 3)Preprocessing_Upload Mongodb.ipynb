{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3)Preprocessing&Upload Mongodb.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["##**Scarichiamo le librerie necessarie al funzionamento di pymongo**"],"metadata":{"id":"OvQpryF6HHfv"}},{"cell_type":"code","source":["!pip install pymongo[srv]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655282857905,"user_tz":-120,"elapsed":4352,"user":{"displayName":"Tommaso Strada","userId":"07272469469703036442"}},"outputId":"3bf3d01b-3b95-4b45-f00c-c291928f31d1","id":"r2ELC0HAHF9u"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pymongo[srv] in /usr/local/lib/python3.7/dist-packages (4.1.1)\n","Collecting dnspython<3.0.0,>=1.16.0\n","  Downloading dnspython-2.2.1-py3-none-any.whl (269 kB)\n","\u001b[K     |████████████████████████████████| 269 kB 5.1 MB/s \n","\u001b[?25hInstalling collected packages: dnspython\n","Successfully installed dnspython-2.2.1\n"]}]},{"cell_type":"code","source":["!pip install dnspython"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655283012650,"user_tz":-120,"elapsed":3191,"user":{"displayName":"Tommaso Strada","userId":"07272469469703036442"}},"outputId":"0cbd7c5d-cb74-41ba-ce38-7c1d26481fb3","id":"FzTXCMMPHF9w"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: dnspython in /usr/local/lib/python3.7/dist-packages (2.2.1)\n"]}]},{"cell_type":"code","source":["!pip install pymongo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655283016043,"user_tz":-120,"elapsed":3407,"user":{"displayName":"Tommaso Strada","userId":"07272469469703036442"}},"outputId":"44266527-7674-4725-c956-926add40266f","id":"IxBAwrKXHF9x"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (4.1.1)\n"]}]},{"cell_type":"code","source":["!apt-get install mongodb\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655283018515,"user_tz":-120,"elapsed":2500,"user":{"displayName":"Tommaso Strada","userId":"07272469469703036442"}},"outputId":"13734882-8aec-42a6-b2d9-c8868a2d1b46","id":"ToAALXcEHF9y"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","mongodb is already the newest version (1:3.6.3-0ubuntu1.4).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n"]}]},{"cell_type":"code","source":["!service mongodb start"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655283018516,"user_tz":-120,"elapsed":13,"user":{"displayName":"Tommaso Strada","userId":"07272469469703036442"}},"outputId":"9ff5494a-61e0-4069-b144-5c04f6c5edea","id":"GRFa76q7HF9z"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" * Starting database mongodb\n","   ...done.\n"]}]},{"cell_type":"markdown","source":["# Preprocessing Scraping news"],"metadata":{"id":"aSmLIF_Jw3uI"}},{"cell_type":"markdown","metadata":{"id":"5_aOjulO2RD5"},"source":["##**Elimina tuple che hanno 'twitter_link' e quindi 'twitter_id' vuote**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9944,"status":"ok","timestamp":1655236464512,"user":{"displayName":"Tommaso Strada","userId":"07272469469703036442"},"user_tz":-120},"id":"NAA_uQrLEnc0","outputId":"35c2e887-d531-4444-e40f-1b7e33cd214e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import pandas as pd\n","from google.colab import files\n","from google.colab import drive\n","import os\n","import glob\n","\n","# Colleghiamoci a Mydrive\n","drive.mount('/content/drive')\n","\n","# Cartella da cui importare file(JSON) = '/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/JSON SCRAPING TOM'   \n","# Cartella da cui esportare file(JSON) puliti = '/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/JSON SCRAPING CLEAN TOM'   \n","\n","# Spostati nella cartella contenente tutti i dataset di scraping\n","path = '/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/JSON SCRAPING'\n","os.chdir(path)\n","\n","# Importa tutti i file della cartella\n","scrape_list = [i for i in glob.glob('*.json')] #[:7], [8:14] \n","\n","# Inseriscili in un dizionario\n","diz = {'{}'.format((m).split(' scraping')[0]): pd.read_json(m, orient='records') for m in scrape_list}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9aDPaLnY7tVx"},"outputs":[],"source":["# # Elimina righe e dataset vuoti\n","for el in diz:\n","  diz[el] = diz[el][diz[el]['twitter_id'].apply(lambda x: len(x)) != 0]\n","  if len(diz[el]) != 0:\n","    diz[el].to_json(('/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/JSON SCRAPING CLEAN TOM/{} scraping_news_clean.json').format(el), orient='records')\n"]},{"cell_type":"markdown","source":["#Preprocessing MongoDB"],"metadata":{"id":"9Oqx9Ph2xOXh"}},{"cell_type":"markdown","source":["##**Importa i dataset di scraping e twitter**"],"metadata":{"id":"OMpYv-hNwJ3h"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1915,"status":"ok","timestamp":1655293704495,"user":{"displayName":"Tommaso Strada","userId":"07272469469703036442"},"user_tz":-120},"outputId":"7bb61bc0-b1ac-4803-d71e-7da5c5b2c660","id":"_LLkBqNPwI29"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","['Final_post_Spider-Man: No Way Home Twitter.json', 'Final_post_Eternals Twitter.json', 'Final_post_Shang-Chi and the Legend of the Ten Rings Twitter.json', 'Final_post_Black Widow Twitter.json', 'Final_post_Avengers: Endgame Twitter.json', 'Final_post_Ant-Man and the Wasp Twitter.json', 'Final_post_Captain Marvel Twitter.json', 'Final_post_Avengers: Infinity War Twitter.json', 'Final_post_Black Panther Twitter.json', 'Final_post_Thor: Ragnarok Twitter.json', 'Final_post_Iron Man 3 Twitter.json', 'Final_post_Captain America: Civil War Twitter.json', 'Final_post_Spider-Man: Homecoming Twitter.json', 'Final_post_Ant-Man Twitter.json', 'Final_post_Avengers: Age of Ultron Twitter.json', 'Final_post_Doctor Strange Twitter.json', 'Final_post_Guardians of Galaxy Vol.2 Twitter.json', 'Final_post_Spider-Man: Far From Home Twitter.json', 'Final_post_Doctor Strange in the Multiverse of Madness Twitter.json']\n","['Doctor Strange in the Multiverse of Madness Twitter scraping_news_clean.json', 'Spider-Man: No Way Home Twitter scraping_news_clean.json', 'Eternals Twitter scraping_news_clean.json', 'Shang-Chi and the Legend of the Ten Rings Twitter scraping_news_clean.json', 'Black Widow Twitter scraping_news_clean.json', 'Spider-Man: Far From Home Twitter scraping_news_clean.json', 'Avengers: Endgame Twitter scraping_news_clean.json', 'Captain Marvel Twitter scraping_news_clean.json', 'Ant-Man and the Wasp Twitter scraping_news_clean.json', 'Avengers: Infinity War Twitter scraping_news_clean.json', 'Black Panther Twitter scraping_news_clean.json', 'Thor: Ragnarok Twitter scraping_news_clean.json', 'Spider-Man: Homecoming Twitter scraping_news_clean.json', 'Iron Man 3 Twitter scraping_news_clean.json', 'Avengers: Age of Ultron Twitter scraping_news_clean.json', 'Ant-Man Twitter scraping_news_clean.json', 'Captain America: Civil War Twitter scraping_news_clean.json', 'Doctor Strange Twitter scraping_news_clean.json', 'Guardians of Galaxy Vol.2 Twitter scraping_news_clean.json']\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import re\n","from google.colab import files\n","from google.colab import drive\n","import os\n","import pymongo\n","import glob\n","import json\n","\n","# Colleghiamoci a Mydrive\n","drive.mount('/content/drive')\n","\n","# Cartella in cui importare file (JSON) = '/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/JSON SCRAPING CLEAN TOM'  \n","# Cartella in cui esportare file post twitter (JSON) = '/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/JSON TWITTER TOM'  \n","\n","path_scraping = '/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/JSON SCRAPING CLEAN'\n","path_twitter= '/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/JSON TWITTER'\n","# Spostati nella cartella contenente tutti i dataset di scraping\n","os.chdir(path_scraping)\n","\n","# Importa tutti i file della cartella \n","scrape_list = [i for i in glob.glob('*.json')] \n","\n","# Inseriscili in un dizionario \n","diz_scraping = {'{}'.format((m).split(' Twitter')[0]): json.loads(open(m).read()) for m in scrape_list}\n","\n","# Spostati nella cartella contenente tutti i dataset di Twitter\n","os.chdir(path_twitter)\n","\n","# Importa tutti i file della cartella \n","tw_list = [h for h in glob.glob('*.json')] #[:7], [8:14] \n","\n","# Inseriscili in un dizionario\n","diz_tw = {'{}'.format(((n).split('Final_post_')[1]).split(' Twitter')[0]): json.loads(open(n).read()) for n in tw_list}\n","\n","print(tw_list)\n","print(scrape_list)\n"]},{"cell_type":"markdown","source":["**Per ogni film creiamo il campo 'twitter id' che contenga la tupla del dataset twitter corrispondente a quell'id**"],"metadata":{"id":"gDd6nJHc0lp9"}},{"cell_type":"code","source":["for film in diz_tw:\n","  for row in range(len(diz_scraping[film])):\n","    f = [diz_tw[film][el]     # metti il contenuto di diz_tw della riga el Se: *\n","         for n in range(len(diz_scraping[film][row]['twitter_id']))   # per ogni n elemento della colonna 'twitter_id' alla riga row\n","         for el in range(len(diz_tw[film]))       # per ogni riga di diz_tw \n","         if diz_tw[film][el]['Tweet_id'] == diz_scraping[film][row]['twitter_id'][n]]   # * tweet_id in diz_tw == all'el-esimo twitter_id di diz_scraping     \n","    diz_scraping[film][row]['twitter_id'] = f\n"],"metadata":{"id":"W3P1dxlEZ6IQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["diz_scraping['Doctor Strange in the Multiverse of Madness'][0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9iXy3TnPqkC_","executionInfo":{"status":"ok","timestamp":1655293796380,"user_tz":-120,"elapsed":7,"user":{"displayName":"Tommaso Strada","userId":"07272469469703036442"}},"outputId":"ec6702ba-e05a-4706-9294-4fad3d1efc3c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'link': 'https://www.koimoi.com/hollywood-news/benedict-cumberbatch-doctor-strange-in-the-multiverse-of-madness-trailer-leaked-you-cannot-even-guess-whos-making-a-big-cameo/',\n"," 'published': '15/12/2021',\n"," 'title': 'Benedict Cumberbatch’s Doctor Strange In The Multiverse Of Madness Trailer Leaked & You Cannot Even Guess Who’s Making A Big Cameo - Koimoi',\n"," 'twitter_id': [],\n"," 'twitter_links': ['https://twitter.com/MartinL91232906/status/1470967146452815881']}"]},"metadata":{},"execution_count":73}]},{"cell_type":"markdown","source":["##Caricamento documenti su database MongoDb"],"metadata":{"id":"yHFLIUgVHSz6"}},{"cell_type":"markdown","source":["##**Costruiamo un documento per ogni film**"],"metadata":{"id":"3KRjlIrO15go"}},{"cell_type":"code","source":["import pymongo \n","client = pymongo.MongoClient('mongodb+srv://TStrada:DataMan@clusterproject.laukf.mongodb.net/test')\n","db = client['Film-Marvel']\n","phase1 = db['Phase1']\n","phase2 = db['Phase2']\n","phase3 = db['Phase3']\n","phase4 = db['Phase4']\n"],"metadata":{"id":"T88TXtHq_Lrf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Phase 1**"],"metadata":{"id":"wkN5hmoB9Lan"}},{"cell_type":"code","source":["# Phase 1\n","d1 = {'Film': 'Iron Man', 'Release Date': '2008-05-02', 'Main Character': 'Iron Man', 'Articles and Tweets': np.nan}\n","d2 = {'Film': 'The Incredible Hulk', 'Release Date': '2008-06-13', 'Main Character': 'Hulk', 'Articles and Tweets': np.nan}\n","d3 = {'Film': 'Iron Man 2', 'Release Date': '2010-05-07', 'Main Character': 'Iron Man', 'Articles and Tweets': np.nan}\n","d4 = {'Film': 'Thor', 'Release Date': '2011-05-06', 'Main Character': 'Thor', 'Articles and Tweets': np.nan}\n","d5 = {'Film': 'Captain America: The First Avenger', 'Release Date': '2011-07-22', 'Main Character': 'Captain America', 'Articles and Tweets': np.nan}\n","d6 = {'Film': 'The Avengers', 'Release Date': '2012-05-04', 'Main Character': 'Avengers', 'Articles and Tweets': np.nan}\n","\n","for film in diz_scraping:\n","  if (d1['Film'] == film):\n","    d1['Articles and Tweets'] = diz_scraping[film]\n","    print(film)\n","    \n","  elif (d2['Film'] == film):\n","    d2['Articles and Tweets'] = diz_scraping[film]\n","    print(film)\n","\n","  elif( d3['Film'] == film):\n","    d3['Articles and Tweets'] = diz_scraping[film]\n","    print(film)\n","\n","  elif(d4['Film'] == film):\n","    d3['Articles and Tweets'] = diz_scraping[film]\n","    print(film)\n","\n","  elif(d5['Film'] == film):\n","    d3['Articles and Tweets'] = diz_scraping[film]\n","    print(film)\n","\n","  elif(d6['Film'] == film):\n","    d6['Articles and Tweets'] = diz_scraping[film]\n","    print(film)\n","\n","# Salva documenti\n","list_doc = [d1, d2, d3, d4, d5, d6]\n","for doc in list_doc:\n","  with open('/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/Mongodb DF (JSON)/Phase1:{}.json'.format(doc['Film']), 'w') as file:\n","    json.dump(doc, file)\n","  \n","# Carica su mongodb\n","db.phase1.insert_one(d1)   \n","db.phase1.insert_one(d2)   \n","db.phase1.insert_one(d3)   \n","db.phase1.insert_one(d4)   \n","db.phase1.insert_one(d5)   \n","db.phase1.insert_one(d6)   "],"metadata":{"id":"fO6pv_d-5Nnj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Phase 2**"],"metadata":{"id":"xGAcbjrY93N_"}},{"cell_type":"code","source":["d7 = {'Film': 'Iron Man 3', 'Release Date': '2013-04-14', 'Main Character': 'Iron Man', 'Articles and Tweets': np.nan}\n","d8 = {'Film': 'Thor: The Dark World', 'Release Date': '2013-10-22', 'Main Character': 'Thor', 'Articles and Tweets': np.nan}\n","d9 = {'Film': 'Captain America: The Winter Soldier', 'Release Date': '2014-03-13', 'Main Character': 'Captain America', 'Articles and Tweets': np.nan}\n","d10 = {'Film': 'Guardians of Galaxy', 'Release Date': '2014-08-01', 'Main Character': 'Guardians of Galaxy', 'Articles and Tweets': np.nan}\n","d11 = {'Film': 'Avengers: Age of Ultron', 'Release Date': '2015-05-01', 'Main Character': 'Avengers', 'Articles and Tweets': np.nan}\n","d12 = {'Film': 'Ant-Man', 'Release Date': '2015-07-17', 'Main Character': 'Ant Man', 'Articles and Tweets': np.nan}\n","\n","for film in diz_scraping:\n","  if (d7['Film'] == film):\n","    d7['Articles and Tweets'] = diz_scraping[film]\n","    print(film)\n","    \n","  elif (d8['Film'] == film):\n","    d8['Articles and Tweets'] = diz_scraping[film]\n","    print(film)\n","\n","  elif(d9['Film'] == film):\n","    d9['Articles and Tweets'] = diz_scraping[film]\n","    print(film)\n","\n","  elif(d10['Film'] == film):\n","    d10['Articles and Tweets'] = diz_scraping[film]\n","    print(film)\n","\n","  elif(d11['Film'] == film):\n","    d11['Articles and Tweets'] = diz_scraping[film]\n","    print(film)\n","\n","  elif(d12['Film'] == film):\n","    d12['Articles and Tweets'] = diz_scraping[film]\n","    print(film)\n","\n","# Salva documenti\n","list_doc = [d7, d8, d9, d10, d11, d12]\n","for doc in list_doc:\n","  with open('/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/Mongodb DF (JSON)/Phase2:{}.json'.format(doc['Film']), 'w') as file:\n","    json.dump(doc, file)\n","\n","\n","# Carica su mongodb\n","db.phase2.insert_one(d7)   \n","db.phase2.insert_one(d8)   \n","db.phase2.insert_one(d9)   \n","db.phase2.insert_one(d10)   \n","db.phase2.insert_one(d11)   \n","db.phase2.insert_one(d12)   "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e732552D6O3o","executionInfo":{"status":"ok","timestamp":1655293802149,"user_tz":-120,"elapsed":293,"user":{"displayName":"Tommaso Strada","userId":"07272469469703036442"}},"outputId":"6f963137-9791-4b27-ad63-3381c6da8828"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iron Man 3\n","Avengers: Age of Ultron\n","Ant-Man\n"]}]},{"cell_type":"markdown","source":["**Phase 3**"],"metadata":{"id":"NlfSQZ6m-Xaw"}},{"cell_type":"code","source":["d13 = {'Film': 'Captain America: Civil War', 'Release Date': '2016-05-06', 'Main Character': 'Captain America', 'Articles and Tweets': np.nan}\n","d14 = {'Film': 'Doctor Strange', 'Release Date': '2016-11-04', 'Main Character': 'Doctor Strange', 'Articles and Tweets': np.nan}\n","d15 = {'Film': 'Guardians of Galaxy Vol.2', 'Release Date': '2017-05-05', 'Main Character': 'Guardians of Galaxy', 'Articles and Tweets': np.nan}\n","d16 = {'Film': 'Spider-Man: Homecoming', 'Release Date': '2017-07-07', 'Main Character': 'Spider-Man', 'Articles and Tweets': np.nan}\n","d17 = {'Film': 'Thor: Ragnarok', 'Release Date': '2017-10-10', 'Main Character': 'Thor', 'Articles and Tweets': np.nan}\n","d18 = {'Film': 'Black Panther', 'Release Date': '2018-02-16', 'Main Character': 'Black Panther', 'Articles and Tweets': np.nan}\n","d19 = {'Film': 'Avengers: Infinity War', 'Release Date': '2018-04-27', 'Main Character': 'Avengers', 'Articles and Tweets': np.nan}\n","d20 = {'Film': 'Ant-Man and the Wasp', 'Release Date': '2018-07-06', 'Main Character': 'Ant Man', 'Articles and Tweets': np.nan}\n","d21 = {'Film': 'Captain Marvel', 'Release Date': '2019-03-08', 'Main Character': 'Captain Marvel', 'Articles and Tweets': np.nan}\n","d22 = {'Film': 'Avengers: Endgame', 'Release Date': '2019-04-26', 'Main Character': 'Avengers', 'Articles and Tweets': np.nan}\n","d23 = {'Film': 'Spider-Man: Far From Home', 'Release Date': '2019-07-02', 'Main Character': 'Spider-Man', 'Articles and Tweets': np.nan}\n","\n","for film in diz_scraping:\n","  if(d13['Film'] == film):\n","    d13['Articles and Tweets'] = diz_scraping[film]\n","    print(film)\n","  \n","  elif(d14['Film'] == film):\n","    d14['Articles and Tweets'] = diz_scraping[film]\n","    print(film)\n","  \n","  elif(d15['Film'] == film):\n","    d15['Articles and Tweets'] = diz_scraping[film]\n","    print(film)\n","  \n","  elif(d16['Film'] == film):\n","    d16['Articles and Tweets'] = diz_scraping[film]\n","    print(film)\n","  \n","  elif(d17['Film'] == film):\n","    d17['Articles and Tweets'] = diz_scraping[film]\n","    print(film)\n","  \n","  elif(d18['Film'] == film):\n","    d18['Articles and Tweets'] = diz_scraping[film]\n","    print(film)\n","  \n","  elif(d19['Film'] == film):\n","    d19['Articles and Tweets'] = diz_scraping[film]\n","    print(film)\n","  \n","  elif(d20['Film'] == film):\n","    d20['Articles and Tweets'] = diz_scraping[film]\n","    print(film)\n","  \n","  elif(d21['Film'] == film):\n","    d21['Articles and Tweets'] = diz_scraping[film]\n","    print(film)\n","  \n","  elif(d22['Film'] == film):\n","    d22['Articles and Tweets'] = diz_scraping[film]\n","    print(film)\n","  \n","  elif(d23['Film'] == film):\n","    d23['Articles and Tweets'] = diz_scraping[film]\n","    print(film)\n","\n","# Salva documenti\n","list_doc = [d13, d14, d15, d16, d17, d19, d20, d21, d22, d23]\n","for doc in list_doc:\n","  with open('/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/Mongodb DF (JSON)/Phase3:{}.json'.format(doc['Film']), 'w') as file:\n","    json.dump(doc, file)\n","\n","# Carica su mongodb\n","db.phase3.insert_one(d13)   \n","db.phase3.insert_one(d14)   \n","db.phase3.insert_one(d15)   \n","db.phase3.insert_one(d16)   \n","db.phase3.insert_one(d17)   \n","db.phase3.insert_one(d18) \n","db.phase3.insert_one(d19)   \n","db.phase3.insert_one(d20)   \n","db.phase3.insert_one(d21)   \n","db.phase3.insert_one(d22)   \n","db.phase3.insert_one(d23) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dDcgFO12-ZTe","executionInfo":{"status":"ok","timestamp":1655294167994,"user_tz":-120,"elapsed":1473,"user":{"displayName":"Tommaso Strada","userId":"07272469469703036442"}},"outputId":"2b561e0c-1f33-4794-e81e-54bd870ddd61"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Spider-Man: Far From Home\n","Avengers: Endgame\n","Captain Marvel\n","Ant-Man and the Wasp\n","Avengers: Infinity War\n","Black Panther\n","Thor: Ragnarok\n","Spider-Man: Homecoming\n","Captain America: Civil War\n","Doctor Strange\n","Guardians of Galaxy Vol.2\n"]},{"output_type":"execute_result","data":{"text/plain":["<pymongo.results.InsertOneResult at 0x7f4cace89f90>"]},"metadata":{},"execution_count":83}]},{"cell_type":"markdown","source":["**Phase 4**"],"metadata":{"id":"2WZG0eN9_Qut"}},{"cell_type":"code","source":["d24 = {'Film': 'Black Widow', 'Release Date': '2021-07-09', 'Main Character': 'Black Widow', 'Articles and Tweets': np.nan}\n","d25 = {'Film': 'Shang-Chi and the Legend of the Ten Rings', 'Release Date': '2021-09-03', 'Main Character': 'Shang-Chi', 'Articles and Tweets': np.nan}\n","d26 = {'Film': 'Eternals', 'Release Date': '2021-11-05', 'Main Character': 'Eternals', 'Articles and Tweets': np.nan}\n","d27 = {'Film': 'Spider-Man: No Way Home', 'Release Date': '2021-12-14', 'Main Character': 'Spider-Man', 'Articles and Tweets': np.nan}\n","d28 = {'Film': 'Doctor Strange in the Multiverse of Madness', 'Release Date': '2022-05-06', 'Main Character': 'Doctor Strange', 'Articles and Tweets': np.nan}\n"," \n","for film in diz_scraping:\n","  if(d24['Film'] == film):\n","    d24['Articles and Tweets'] = diz_scraping[film]\n","    print(film)\n","  \n","  elif(d25['Film'] == film):\n","    d25['Articles and Tweets'] = diz_scraping[film]\n","    print(film)\n","  \n","  elif(d26['Film'] == film):\n","    d26['Articles and Tweets'] = diz_scraping[film]\n","    print(film)\n","  \n","  elif(d27['Film'] == film):\n","    d27['Articles and Tweets'] = diz_scraping[film]\n","    print(film)\n","\n","  elif(d28['Film'] == film):\n","    d28['Articles and Tweets'] = diz_scraping[film]\n","    print(film)\n","\n","# Salva documenti\n","list_doc = [d24, d25, d26, d27, d28]\n","for doc in list_doc:\n","  with open('/content/drive/MyDrive/DATA SCIENCE/Progetti I anno/Dataset/Mongodb DF (JSON)/Phase4:{}.json'.format(doc['Film']), 'w') as file:\n","    json.dump(doc, file)\n","\n","# Carica su mongodb\n","db.phase4.insert_one(d24)   \n","db.phase4.insert_one(d25)   \n","db.phase4.insert_one(d26)   \n","db.phase4.insert_one(d27)   \n","db.phase4.insert_one(d28)   "],"metadata":{"id":"38Ii9_p92Afh","executionInfo":{"status":"ok","timestamp":1655293805507,"user_tz":-120,"elapsed":587,"user":{"displayName":"Tommaso Strada","userId":"07272469469703036442"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3bce58a5-8a6e-4382-8787-b632abeee027"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Doctor Strange in the Multiverse of Madness\n","Spider-Man: No Way Home\n","Eternals\n","Shang-Chi and the Legend of the Ten Rings\n","Black Widow\n"]}]}]}